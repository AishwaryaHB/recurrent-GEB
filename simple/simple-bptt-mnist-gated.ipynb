{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede197e5",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c606816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b22af",
   "metadata": {},
   "source": [
    "#### Test for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba2ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU, training on CPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('No GPU, training on CPU')\n",
    "    dev = torch.device('cpu')\n",
    "else:\n",
    "    print('GPU found, training on GPU')\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2b541",
   "metadata": {},
   "source": [
    "#### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d548a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure batch_size = 1 for now!!\n",
    "\n",
    "def load_mnist(batch_size=1, shuffle_train=True):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_set = torchvision.datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(\"../data\", train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a3feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader, mnist_test_loader = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94238cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(mnist_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9de244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change to appropriate shapes!!\n",
    "image_batch = torch.squeeze(image_batch).reshape(1,-1)\n",
    "image_batch = image_batch.repeat(10,1)\n",
    "label_batch = F.one_hot(label_batch,10).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f6be7",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b227d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule:\n",
    "    \"\"\"An RNN cell responsible for a single timestep.\n",
    "\n",
    "    Args:\n",
    "        inp_dim (int): Input size.\n",
    "        hid_dim (int): Hidden size.\n",
    "        out_dim (int): Output size.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, hid_dim, out_dim):\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        ## Wih, Whh, Woh are the parameters, so we set requires_grad=True\n",
    "        self.Wih = torch.empty(hid_dim, inp_dim, requires_grad=True)\n",
    "        self.Whh = torch.empty(hid_dim, hid_dim, requires_grad=True)\n",
    "        self.Woh = torch.empty(out_dim, hid_dim, requires_grad=True)\n",
    "\n",
    "        ## These are the gradients on Wih, Whh, and Woh computed manually\n",
    "        ## Will be compared to the gradients computed by PyTorch\n",
    "        self.Wih_grad = torch.zeros_like(self.Wih)\n",
    "        self.Whh_grad = torch.zeros_like(self.Whh)\n",
    "        self.Woh_grad = torch.zeros_like(self.Woh)\n",
    "        \n",
    "        ## Gating vector\n",
    "        self.tvec = torch.zeros(hid_dim)\n",
    "        t_half = torch.randint(0, 2, (1, hid_dim//2)).float()*2 - 1\n",
    "        self.tvec[::2] = t_half\n",
    "        self.tvec[1::2] = -t_half\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\n",
    "\n",
    "        The parameters will be initialized from the uniform\n",
    "        distribution U(-0.1, 0.1).\n",
    "        \"\"\"\n",
    "        s = 0.1  # larger value may make the gradients explode\n",
    "        torch.nn.init.uniform_(self.Wih, -s, s)\n",
    "        torch.nn.init.uniform_(self.Whh, 0, s)\n",
    "        torch.nn.init.uniform_(self.Woh, 0, s)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Set the gradients to zero.\"\"\"\n",
    "        self.Wih_grad.zero_()\n",
    "        self.Whh_grad.zero_()\n",
    "        self.Woh_grad.zero_()\n",
    "\n",
    "    def forward(self, x, hp):\n",
    "        \"\"\"Perform the forward computation.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input at the current timestep.\n",
    "            hp (Tensor): Hidden state at the previous timestep.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output at the current timestep.\n",
    "            Tensor: Hidden state at the current timestep.\n",
    "        \"\"\"\n",
    "        _, h, _, y = self._get_internals(x, hp)\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, y_grad, rn_grad, x, hp):\n",
    "        \"\"\"Perform the backward computation.\n",
    "        \n",
    "        Args:\n",
    "            y_grad (Tensor): Gradient on output at the current timestep.\n",
    "            rn_grad (Tensor): Gradient on vector r at the next timestep.\n",
    "            x (Tensor): Input at the current timestep that was passed to `forward`.\n",
    "            hp (Tensor): Hidden state at the previous timestep that was passed to `forward`.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Gradient on vector r at the current timestep.\n",
    "        \"\"\"\n",
    "        # Get internal vectors r, h, and s from forward computation\n",
    "        r, h, s, _ = self._get_internals(x, hp)\n",
    "\n",
    "        s_grad = y_grad * torch.sigmoid(s) * (1-torch.sigmoid(s)) ## note manual differentiation!!\n",
    "        h_grad = self.Woh.t().matmul(s_grad) + self.Whh.t().matmul(rn_grad)\n",
    "        r_grad = h_grad * ((self.tvec*r)>0)*1 ## note manual differentiation!!\n",
    "\n",
    "        # Parameter gradients are accumulated\n",
    "        self.Wih_grad += r_grad.view(-1, 1).matmul(x.view(1, -1))\n",
    "        self.Whh_grad += r_grad.view(-1, 1).matmul(hp.view(1, -1)) ##changed\n",
    "        self.Woh_grad += s_grad.view(-1, 1).matmul(h.view(1, -1)) ##changed\n",
    "\n",
    "        return r_grad\n",
    "    \n",
    "    def _get_internals(self, x, hp):\n",
    "        # Actual forward computations\n",
    "        r = self.Wih.matmul(x) + self.Whh.matmul(hp)\n",
    "        h = ((self.tvec*r)>0)*r\n",
    "        s = self.Woh.matmul(h)\n",
    "        y = torch.sigmoid(s)\n",
    "        \n",
    "        return r, h, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d878f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, cell):\n",
    "        self.cell = cell\n",
    "    \n",
    "    def forward(self, xs, h0):\n",
    "        \"\"\"Perform the forward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep. The\n",
    "                first dimension corresponds to the number of timesteps.\n",
    "            h0 (Tensor): Initial hidden state.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: 2-D tensor of outputs for each timestep. The first\n",
    "                dimension corresponds to the number of timesteps.\n",
    "            Tensor: 2-D tensor of hidden states for each timestep plus\n",
    "                `h0`. The first dimension corresponds to the number of\n",
    "                timesteps.\n",
    "        \"\"\"\n",
    "        ys, hs = [], [h0]\n",
    "        for x in xs:\n",
    "            y, h = self.cell.forward(x, hs[-1])\n",
    "            ys.append(y)\n",
    "            hs.append(h)\n",
    "        return torch.stack(ys), torch.stack(hs)\n",
    "    \n",
    "    def backward(self, ys_grad, xs, hs):\n",
    "        \"\"\"Perform the backward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            ys_grad (Tensor): 2-D tensor of the gradients on outputs\n",
    "                for each timestep. The first dimension corresponds to\n",
    "                the number of timesteps.\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep that\n",
    "                was passed to `forward`.\n",
    "            hs (Tensor): 2-D tensor of hidden states that is returned\n",
    "                by `forward`.\n",
    "        \"\"\"\n",
    "        # For the last timestep, the gradient on r is zero\n",
    "        rn_grad = torch.zeros(self.cell.hid_dim)\n",
    "\n",
    "        for y_grad, x, hp in reversed(list(zip(ys_grad, xs, hs))):\n",
    "            rn_grad = cell.backward(y_grad, rn_grad, x, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef10e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 100\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6aca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = RNNModule(input_dim, hidden_dim, output_dim)\n",
    "rnn = RNN(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590043bd",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2727e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_cross_entropy(predictions, targets, epsilon=1e-15):\n",
    "#     \"\"\"\n",
    "#     Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "#     and predictions. \n",
    "#     Input: predictions (N, k) ndarray\n",
    "#            targets (N, k) ndarray        \n",
    "#     Returns: scalar\n",
    "#     \"\"\"\n",
    "#     predictions = torch.clip(predictions, epsilon, 1. - epsilon)\n",
    "#     N = predictions.shape[0]\n",
    "#     ce = -torch.sum(targets*torch.log(predictions+1e-12))/N\n",
    "    \n",
    "#     return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e780b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(ys, ts):\n",
    "    return 0.5 * torch.sum((ys - ts)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33eb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note -- changed!!\n",
    "xs = image_batch\n",
    "hp = torch.zeros(cell.hid_dim)\n",
    "ts = label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31f8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc46c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)\n",
    "loss = compute_loss(ys, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe4487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e7380",
   "metadata": {},
   "source": [
    "#### PyTorch gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "053ad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient of cross entropy w.r.t. yhat_k = (1/yhat_k)*y_k\n",
    "## gradient of mse w.r.t. y_hat =   y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac4d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab2b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4054d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8bce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d557c5",
   "metadata": {},
   "source": [
    "#### Manual gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a3eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is obtained from our loss function\n",
    "# ys_grad = torch.matmul(label_batch,ys)\n",
    "ys_grad = ys - ts\n",
    "\n",
    "with torch.no_grad():  # required so PyTorch won't raise error\n",
    "    rnn.cell.zero_grad()\n",
    "    rnn.backward(ys_grad, xs, hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "601539f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86292cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23dc2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fdad959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -5.9605e-08, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  2.9104e-11,  0.0000e+00, -3.7253e-09,\n",
       "          0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00, -1.1921e-07,\n",
       "          0.0000e+00,  5.8208e-11,  1.4901e-08, -2.9802e-08, -1.1921e-07,\n",
       "          0.0000e+00,  0.0000e+00,  2.9104e-11,  0.0000e+00, -1.1642e-10,\n",
       "          0.0000e+00,  7.4506e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -2.3283e-10,  5.9605e-08, -1.4901e-08,  0.0000e+00, -2.9802e-08,\n",
       "          0.0000e+00, -3.7253e-09, -2.9802e-08,  0.0000e+00,  1.4901e-08,\n",
       "         -9.3132e-10,  1.1921e-07,  1.4552e-11,  7.2760e-12,  0.0000e+00,\n",
       "          0.0000e+00, -2.9802e-08,  0.0000e+00,  2.9104e-11,  0.0000e+00,\n",
       "          0.0000e+00, -5.9605e-08,  0.0000e+00, -7.4506e-09,  5.9605e-08,\n",
       "          0.0000e+00, -2.9802e-08, -2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  5.9605e-08,  7.2760e-12, -2.9802e-08,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -3.7253e-09,  0.0000e+00, -2.9802e-08,\n",
       "          0.0000e+00, -2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          2.9802e-08, -7.4506e-09,  0.0000e+00,  2.9104e-11,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4552e-11,\n",
       "          2.9104e-11,  0.0000e+00, -1.4901e-08,  7.4506e-09,  0.0000e+00,\n",
       "          0.0000e+00, -5.9605e-08,  2.9802e-08, -2.9802e-08,  0.0000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.cell.Woh_grad - rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25345d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.8190e-12,  ...,  0.0000e+00,\n",
       "          9.0949e-13,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00, -1.8626e-09,  ...,  9.3132e-10,\n",
       "         -9.3132e-10,  1.8626e-09],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00, -3.4925e-10,  ...,  2.3283e-10,\n",
       "         -1.7462e-10,  4.6566e-10],\n",
       "        [ 0.0000e+00,  0.0000e+00, -9.3132e-10,  ...,  4.6566e-10,\n",
       "          0.0000e+00,  9.3132e-10],\n",
       "        [ 0.0000e+00,  0.0000e+00, -5.8208e-11,  ...,  2.9104e-11,\n",
       "         -5.8208e-11,  1.1642e-10]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.cell.Whh_grad - rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e38c20dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-4.5475e-13, -4.5475e-13, -4.5475e-13,  ..., -4.5475e-13,\n",
       "         -4.5475e-13, -4.5475e-13],\n",
       "        [ 3.7253e-09,  3.7253e-09,  3.7253e-09,  ...,  3.7253e-09,\n",
       "          3.7253e-09,  3.7253e-09],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.7253e-09,  3.7253e-09,  3.7253e-09,  ...,  3.7253e-09,\n",
       "          3.7253e-09,  3.7253e-09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.cell.Wih_grad - rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c49b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1358ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a9045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
