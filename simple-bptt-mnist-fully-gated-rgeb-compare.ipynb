{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede197e5",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c606816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b22af",
   "metadata": {},
   "source": [
    "#### Test for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba2ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU, training on CPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('No GPU, training on CPU')\n",
    "    dev = torch.device('cpu')\n",
    "else:\n",
    "    print('GPU found, training on GPU')\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2b541",
   "metadata": {},
   "source": [
    "#### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d548a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure batch_size = 1 for now!!\n",
    "\n",
    "def load_mnist(batch_size=1, shuffle_train=True):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_set = torchvision.datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(\"../data\", train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a3feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader, mnist_test_loader = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94238cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(mnist_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cca50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b270e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 100\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a36b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gating vector\n",
    "tvec = torch.zeros(n_classes,hidden_dim)\n",
    "for ii in range(n_classes):\n",
    "    t_half = torch.randint(0, 2, (1, hidden_dim//2)).float()*2 - 1\n",
    "    tvec[ii,::2] = t_half\n",
    "    tvec[ii,1::2] = -t_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9de244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change to appropriate shapes!!\n",
    "image_batch = torch.squeeze(image_batch).view(1,-1)\n",
    "image_batch = image_batch.repeat(n_classes,1)\n",
    "label_batch = F.one_hot(label_batch,n_classes).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f6be7",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b227d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule:\n",
    "    \"\"\"An RNN cell responsible for a single timestep.\n",
    "\n",
    "    Args:\n",
    "        inp_dim (int): Input size.\n",
    "        hid_dim (int): Hidden size.\n",
    "        out_dim (int): Output size.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, hid_dim, out_dim,t_vec):\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.tvec = t_vec\n",
    "        n_classes = t_vec.shape[0] ## NOTE: Edit as needed!!\n",
    "\n",
    "        ## Wih, Whh, Woh are the parameters, so we set requires_grad=True\n",
    "        self.Wih = torch.empty(hid_dim, inp_dim, requires_grad=True)\n",
    "        self.Whh = torch.empty(hid_dim, hid_dim, requires_grad=True)\n",
    "        self.Woh = torch.empty(out_dim, hid_dim, requires_grad=True)\n",
    "\n",
    "        ## These are the gradients on Wih, Whh, and Woh computed manually\n",
    "        ## Will be compared to the gradients computed by PyTorch\n",
    "        self.Wih_grad = torch.zeros_like(self.Wih)\n",
    "        self.Whh_grad = torch.zeros_like(self.Whh)\n",
    "        self.Woh_grad = torch.zeros_like(self.Woh)\n",
    "        \n",
    "        self.Wih_grad_geb = torch.zeros_like(self.Wih)\n",
    "        self.Whh_grad_geb = torch.zeros_like(self.Whh)\n",
    "        self.Woh_grad_geb = torch.zeros_like(self.Woh)\n",
    "        \n",
    "        self.Wih_grad_all = {}\n",
    "        self.Whh_grad_all = {}\n",
    "        self.Woh_grad_all = {}\n",
    "        \n",
    "        self.Wih_grad_geb_all = {}\n",
    "        self.Whh_grad_geb_all = {}\n",
    "        self.Woh_grad_geb_all = {}\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\n",
    "\n",
    "        The parameters will be initialized from the uniform\n",
    "        distribution U(-0.1, 0.1).\n",
    "        \"\"\"\n",
    "        s = 0.05  # larger value may make the gradients explode/vanish\n",
    "        torch.nn.init.uniform_(self.Wih, -s, s)\n",
    "        torch.nn.init.uniform_(self.Whh, 0, s)\n",
    "        torch.nn.init.uniform_(self.Woh, 0, s)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Set the gradients to zero.\"\"\"\n",
    "        self.Wih_grad.zero_()\n",
    "        self.Whh_grad.zero_()\n",
    "        self.Woh_grad.zero_()\n",
    "        \n",
    "        self.Wih_grad_geb.zero_()\n",
    "        self.Whh_grad_geb.zero_()\n",
    "        self.Woh_grad_geb.zero_()\n",
    "\n",
    "    def forward(self, x, hp, kk):\n",
    "        \"\"\"Perform the forward computation.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input at the current timestep.\n",
    "            hp (Tensor): Hidden state at the previous timestep.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output at the current timestep.\n",
    "            Tensor: Hidden state at the current timestep.\n",
    "        \"\"\"\n",
    "        _, _, h, _, y = self._get_internals(x, hp, kk)\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, y_grad, rn_grad, x, hp, kk):\n",
    "        \"\"\"Perform the backward computation.\n",
    "        \n",
    "        Args:\n",
    "            y_grad (Tensor): Gradient on output at the current timestep.\n",
    "            rn_grad (Tensor): Gradient on vector r at the next timestep.\n",
    "            x (Tensor): Input at the current timestep that was passed to `forward`.\n",
    "            hp (Tensor): Hidden state at the previous timestep that was passed to `forward`.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Gradient on vector r at the current timestep.\n",
    "        \"\"\"\n",
    "        n_classes = y_grad.shape[0]\n",
    "        \n",
    "        # Get internal vectors r, h, and s from forward computation\n",
    "        rint, r, h, s, _ = self._get_internals(x, hp, kk)\n",
    "\n",
    "        ## BPTT calculations\n",
    "        s_grad = y_grad * torch.sigmoid(s) * (1-torch.sigmoid(s)) ## note manual differentiation!!\n",
    "        h_grad = self.Woh.t().matmul(s_grad) + self.Whh.t().matmul(rn_grad)\n",
    "        r_grad = h_grad * ((self.tvec[kk]*r)>0)*1 ## note manual differentiation!!\n",
    "        rint_grad = r_grad*((self.tvec[kk]*rint)>0)*1 ## note manual differentiation\n",
    "        \n",
    "        # Parameter gradients are accumulated\n",
    "        self.Wih_grad += rint_grad.view(-1, 1).matmul(x.view(1, -1))\n",
    "        self.Whh_grad += r_grad.view(-1, 1).matmul(hp.view(1, -1))\n",
    "        self.Woh_grad += s_grad.view(-1, 1).matmul(h.view(1, -1))\n",
    "\n",
    "        self.Wih_grad_all[kk] = r_grad.view(-1, 1).matmul(x.view(1, -1))\n",
    "        self.Whh_grad_all[kk] = r_grad.view(-1, 1).matmul(hp.view(1, -1))\n",
    "        self.Woh_grad_all[kk] = s_grad.view(-1, 1).matmul(h.view(1, -1))\n",
    "        \n",
    "        ## R-GEB calculation\n",
    "        \n",
    "        ## Indicator of post synaptic activity\n",
    "        ## Pre-synaptic activation\n",
    "        ## Global error vector\n",
    "\n",
    "        x_y_grad = y_grad*x\n",
    "        rint_post = ((self.tvec[kk]*rint)>0)*1.\n",
    "        \n",
    "        hp_y_grad = y_grad*hp\n",
    "        r_post = ((self.tvec[kk]*r)>0)*1.\n",
    "        \n",
    "        self.Wih_grad_geb += torch.outer(rint_post,x_y_grad) #done!\n",
    "        self.Whh_grad_geb += torch.outer(r_post,hp_y_grad) #done! \n",
    "        self.Woh_grad_geb += s_grad.view(-1,1).matmul(h.view(1, -1)) #done!\n",
    "        \n",
    "        self.Wih_grad_geb_all[kk] = torch.outer(rint_post,x_y_grad)\n",
    "        self.Whh_grad_geb_all[kk] = torch.outer(r_post,hp_y_grad)\n",
    "        self.Woh_grad_geb_all[kk] = s_grad.view(-1,1).matmul(h.view(1, -1))\n",
    "\n",
    "        return r_grad\n",
    "    \n",
    "    def _get_internals(self, x, hp, kk):\n",
    "        # Actual forward computations\n",
    "        rint = self.Wih.matmul(x)\n",
    "        r = ((self.tvec[kk]*rint)>0)*rint + self.Whh.matmul(hp)\n",
    "        h = ((self.tvec[kk]*r)>0)*r\n",
    "        s = self.Woh.matmul(h)\n",
    "        y = torch.sigmoid(s)\n",
    "        \n",
    "        return rint, r, h, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d878f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, cell):\n",
    "        self.cell = cell\n",
    "    \n",
    "    def forward(self, xs, h0):\n",
    "        \"\"\"Perform the forward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep. The\n",
    "                first dimension corresponds to the number of timesteps.\n",
    "            h0 (Tensor): Initial hidden state.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: 2-D tensor of outputs for each timestep. The first\n",
    "                dimension corresponds to the number of timesteps.\n",
    "            Tensor: 2-D tensor of hidden states for each timestep plus\n",
    "                `h0`. The first dimension corresponds to the number of\n",
    "                timesteps.\n",
    "        \"\"\"\n",
    "        ys, hs = [], [h0]\n",
    "        for ii, x in enumerate(xs):\n",
    "            y, h = self.cell.forward(x, hs[-1],ii)\n",
    "            ys.append(y)\n",
    "            hs.append(h)\n",
    "        return torch.stack(ys), torch.stack(hs)\n",
    "    \n",
    "    def backward(self, ys_grad, xs, hs):\n",
    "        \"\"\"Perform the backward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            ys_grad (Tensor): 2-D tensor of the gradients on outputs\n",
    "                for each timestep. The first dimension corresponds to\n",
    "                the number of timesteps.\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep that\n",
    "                was passed to `forward`.\n",
    "            hs (Tensor): 2-D tensor of hidden states that is returned\n",
    "                by `forward`.\n",
    "        \"\"\"\n",
    "        # For the last timestep, the gradient on r is zero\n",
    "        rn_grad = torch.zeros(self.cell.hid_dim)\n",
    "\n",
    "        for ii, (y_grad, x, hp) in enumerate(reversed(list(zip(ys_grad, xs, hs)))):\n",
    "            rn_grad  = cell.backward(y_grad, rn_grad, x, hp, n_classes-ii-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6aca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = RNNModule(input_dim, hidden_dim, output_dim, tvec)\n",
    "rnn = RNN(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590043bd",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2727e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_cross_entropy(predictions, targets, epsilon=1e-15):\n",
    "#     \"\"\"\n",
    "#     Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "#     and predictions. \n",
    "#     Input: predictions (N, k) ndarray\n",
    "#            targets (N, k) ndarray        \n",
    "#     Returns: scalar\n",
    "#     \"\"\"\n",
    "#     predictions = torch.clip(predictions, epsilon, 1. - epsilon)\n",
    "#     N = predictions.shape[0]\n",
    "#     ce = -torch.sum(targets*torch.log(predictions+1e-12))/N\n",
    "    \n",
    "#     return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e780b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(ys, ts):\n",
    "    return 0.5 * torch.sum((ys - ts)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33eb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note -- changed!!\n",
    "xs = image_batch\n",
    "hp = torch.zeros(cell.hid_dim)\n",
    "ts = label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31f8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc46c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)\n",
    "loss = compute_loss(ys, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e7380",
   "metadata": {},
   "source": [
    "#### PyTorch gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "053ad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient of cross entropy w.r.t. yhat_k = (1/yhat_k)*y_k\n",
    "## gradient of mse w.r.t. y_hat = y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac4d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab2b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4054d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a8bce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f07a3",
   "metadata": {},
   "source": [
    "#### Manual gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64a3eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is obtained from our loss function\n",
    "# ys_grad = torch.matmul(label_batch,ys)\n",
    "ys_grad = ys - ts\n",
    "\n",
    "with torch.no_grad():  # required so PyTorch won't raise error\n",
    "    rnn.cell.zero_grad()\n",
    "    rnn.backward(ys_grad, xs, hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093d7a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.arange(1., 101.)\n",
    "v2 = torch.arange(1., 785.)\n",
    "torch.outer(v1,v2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78178369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601539f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86292cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23dc2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fdad959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Woh_grad - rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25345d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh_grad - rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e38c20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad - rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d9c49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Woh_grad - rnn.cell.Woh.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae1358ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Whh_grad - rnn.cell.Whh.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "604a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Wih_grad - rnn.cell.Wih.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "753a1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.sign(rnn.cell.Woh_grad) - torch.sign(rnn.cell.Woh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6578aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (torch.abs(rnn.cell.Woh_grad - rnn.cell.Woh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e84e142f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8893e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.1497e-01,\n",
       "         1.9834e-01],\n",
       "        [5.4449e-02, 4.2452e+00, 5.6638e+00,  ..., 4.2912e+00, 1.3802e+00,\n",
       "         4.5381e+00],\n",
       "        [5.4233e-02, 4.3248e+00, 5.7363e+00,  ..., 4.3535e+00, 1.3749e+00,\n",
       "         4.6104e+00],\n",
       "        ...,\n",
       "        [5.4281e-02, 4.4196e+00, 5.7550e+00,  ..., 4.3694e+00, 1.3447e+00,\n",
       "         4.6750e+00],\n",
       "        [4.8637e-04, 4.3963e+00, 5.3770e+00,  ..., 4.4127e+00, 7.4160e-01,\n",
       "         4.6021e+00],\n",
       "        [5.4283e-02, 1.5096e-01, 2.8599e-01,  ..., 1.2134e-01, 1.3467e+00,\n",
       "         1.3287e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(rnn.cell.Whh_grad - rnn.cell.Whh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6aeb704f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2096, 0.2096, 0.2096,  ..., 0.2096, 0.2096, 0.2096],\n",
       "        [0.3880, 0.3880, 0.3880,  ..., 0.3880, 0.3880, 0.3880],\n",
       "        [0.8388, 0.8388, 0.8388,  ..., 0.8388, 0.8388, 0.8388],\n",
       "        ...,\n",
       "        [0.5822, 0.5822, 0.5822,  ..., 0.5822, 0.5822, 0.5822],\n",
       "        [0.1636, 0.1636, 0.1636,  ..., 0.1636, 0.1636, 0.1636],\n",
       "        [0.1537, 0.1537, 0.1537,  ..., 0.1537, 0.1537, 0.1537]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.abs(rnn.cell.Wih_grad - rnn.cell.Wih_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9e02f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "1\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "2\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "3\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "4\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "5\n",
      "Differences for Whh tensor([0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "6\n",
      "Differences for Whh tensor([0., 1., 2.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "7\n",
      "Differences for Whh tensor([0., 2.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "8\n",
      "Differences for Whh tensor([0., 2.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n",
      "9\n",
      "Differences for Whh tensor([-1.,  0.])\n",
      "Differences for Wih tensor([0.])\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for tt in range(n_classes):\n",
    "    print(tt)\n",
    "    \n",
    "    rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_all[tt]==torch.clamp(rnn.cell.Wih_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_geb_all[tt]==torch.clamp(rnn.cell.Wih_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_all[tt]==torch.clamp(rnn.cell.Whh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_geb_all[tt]==torch.clamp(rnn.cell.Whh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_all[tt]==torch.clamp(rnn.cell.Woh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_geb_all[tt]==torch.clamp(rnn.cell.Woh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    print('Differences for Whh',torch.unique(torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt])))\n",
    "    print('Differences for Wih',torch.unique(torch.sign(rnn.cell.Wih_grad_geb_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt])))\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30739526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c31cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
