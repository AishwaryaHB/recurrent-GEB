{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede197e5",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c606816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b22af",
   "metadata": {},
   "source": [
    "#### Test for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba2ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU, training on CPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('No GPU, training on CPU')\n",
    "    dev = torch.device('cpu')\n",
    "else:\n",
    "    print('GPU found, training on GPU')\n",
    "    dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2b541",
   "metadata": {},
   "source": [
    "#### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d548a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure batch_size = 1 for now!!\n",
    "\n",
    "def load_mnist(batch_size=1, shuffle_train=True):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_set = torchvision.datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(\"../data\", train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a3feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader, mnist_test_loader = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94238cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(mnist_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cca50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c15b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 100\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c8941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gating vector\n",
    "tvec = torch.zeros(n_classes,hidden_dim)\n",
    "for ii in range(n_classes):\n",
    "    t_half = torch.randint(0, 2, (1, hidden_dim//2)).float()*2 - 1\n",
    "    tvec[ii,::2] = t_half\n",
    "    tvec[ii,1::2] = -t_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9de244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change to appropriate shapes!!\n",
    "image_batch = torch.squeeze(image_batch).view(1,-1)\n",
    "image_batch = image_batch.repeat(n_classes,1)\n",
    "label_batch = F.one_hot(label_batch,n_classes).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f6be7",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b227d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule:\n",
    "    \"\"\"An RNN cell responsible for a single timestep.\n",
    "\n",
    "    Args:\n",
    "        inp_dim (int): Input size.\n",
    "        hid_dim (int): Hidden size.\n",
    "        out_dim (int): Output size.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, hid_dim, out_dim,t_vec):\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.tvec = t_vec\n",
    "        n_classes = t_vec.shape[0] ## NOTE: Edit as needed!!\n",
    "\n",
    "        ## Wih, Whh, Woh are the parameters, so we set requires_grad=True\n",
    "        self.Wih = torch.empty(hid_dim, inp_dim, requires_grad=True)\n",
    "        self.Whh = torch.empty(hid_dim, hid_dim, requires_grad=True)\n",
    "        self.Woh = torch.empty(out_dim, hid_dim, requires_grad=True)\n",
    "\n",
    "        ## These are the gradients on Wih, Whh, and Woh computed manually\n",
    "        ## Will be compared to the gradients computed by PyTorch\n",
    "        self.Wih_grad = torch.zeros_like(self.Wih)\n",
    "        self.Whh_grad = torch.zeros_like(self.Whh)\n",
    "        self.Woh_grad = torch.zeros_like(self.Woh)\n",
    "        \n",
    "        self.Wih_grad_geb = torch.zeros_like(self.Wih)\n",
    "        self.Whh_grad_geb = torch.zeros_like(self.Whh)\n",
    "        self.Woh_grad_geb = torch.zeros_like(self.Woh)\n",
    "        \n",
    "        self.Wih_grad_all = {}\n",
    "        self.Whh_grad_all = {}\n",
    "        self.Woh_grad_all = {}\n",
    "        \n",
    "        self.Wih_grad_geb_all = {}\n",
    "        self.Whh_grad_geb_all = {}\n",
    "        self.Woh_grad_geb_all = {}\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\n",
    "\n",
    "        The parameters will be initialized from the uniform\n",
    "        distribution U(-0.1, 0.1).\n",
    "        \"\"\"\n",
    "        s = 0.05  # larger value may make the gradients explode/vanish\n",
    "        torch.nn.init.uniform_(self.Wih, -s, s)\n",
    "        torch.nn.init.uniform_(self.Whh, 0, s)\n",
    "        torch.nn.init.uniform_(self.Woh, 0, s)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Set the gradients to zero.\"\"\"\n",
    "        self.Wih_grad.zero_()\n",
    "        self.Whh_grad.zero_()\n",
    "        self.Woh_grad.zero_()\n",
    "        \n",
    "        self.Wih_grad_geb.zero_()\n",
    "        self.Whh_grad_geb.zero_()\n",
    "        self.Woh_grad_geb.zero_()\n",
    "\n",
    "    def forward(self, x, hp, kk):\n",
    "        \"\"\"Perform the forward computation.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input at the current timestep.\n",
    "            hp (Tensor): Hidden state at the previous timestep.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output at the current timestep.\n",
    "            Tensor: Hidden state at the current timestep.\n",
    "        \"\"\"\n",
    "        _, _, h, _, y = self._get_internals(x, hp, kk)\n",
    "        return y, h\n",
    "\n",
    "    def backward(self, y_grad, rn_grad, x, hp, kk):\n",
    "        \"\"\"Perform the backward computation.\n",
    "        \n",
    "        Args:\n",
    "            y_grad (Tensor): Gradient on output at the current timestep.\n",
    "            rn_grad (Tensor): Gradient on vector r at the next timestep.\n",
    "            x (Tensor): Input at the current timestep that was passed to `forward`.\n",
    "            hp (Tensor): Hidden state at the previous timestep that was passed to `forward`.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Gradient on vector r at the current timestep.\n",
    "        \"\"\"\n",
    "        n_classes = y_grad.shape[0]\n",
    "        \n",
    "        # Get internal vectors r, h, and s from forward computation\n",
    "        rint, r, h, s, _ = self._get_internals(x, hp, kk)\n",
    "\n",
    "        ## BPTT calculations\n",
    "        s_grad = y_grad * torch.sigmoid(s) * (1-torch.sigmoid(s)) ## note manual differentiation!!\n",
    "        h_grad = self.Woh.t().matmul(s_grad) + self.Whh.t().matmul(rn_grad)\n",
    "        r_grad = h_grad * ((self.tvec[kk]*r)>0)*1 ## note manual differentiation!!\n",
    "        rint_grad = r_grad*((self.tvec[kk]*rint)>0)*1 ## note manual differentiation\n",
    "        \n",
    "        # Parameter gradients are accumulated\n",
    "        self.Wih_grad += rint_grad.view(-1, 1).matmul(x.view(1, -1))\n",
    "        self.Whh_grad += r_grad.view(-1, 1).matmul(hp.view(1, -1))\n",
    "        self.Woh_grad += s_grad.view(-1, 1).matmul(h.view(1, -1))\n",
    "\n",
    "        self.Wih_grad_all[kk] = r_grad.view(-1, 1).matmul(x.view(1, -1))\n",
    "        self.Whh_grad_all[kk] = r_grad.view(-1, 1).matmul(hp.view(1, -1))\n",
    "        self.Woh_grad_all[kk] = s_grad.view(-1, 1).matmul(h.view(1, -1))\n",
    "        \n",
    "        ## R-GEB calculation\n",
    "        \n",
    "        ## Indicator of post synaptic activity\n",
    "        ## Pre-synaptic activation\n",
    "        ## Global error vector\n",
    "\n",
    "        x_y_grad = y_grad*x\n",
    "        rint_post = ((self.tvec[kk]*rint)>0)*1.\n",
    "        \n",
    "        hp_y_grad = y_grad*hp\n",
    "        r_post = ((self.tvec[kk]*r)>0)*1.\n",
    "        \n",
    "        self.Wih_grad_geb += torch.outer(rint_post,x_y_grad) #done!\n",
    "        self.Whh_grad_geb += torch.outer(r_post,hp_y_grad) #done! \n",
    "        self.Woh_grad_geb += s_grad.view(-1,1).matmul(h.view(1, -1)) #done!\n",
    "        \n",
    "        self.Wih_grad_geb_all[kk] = torch.outer(rint_post,x_y_grad)\n",
    "        self.Whh_grad_geb_all[kk] = torch.outer(r_post,hp_y_grad)\n",
    "        self.Woh_grad_geb_all[kk] = s_grad.view(-1,1).matmul(h.view(1, -1))\n",
    "\n",
    "        return r_grad\n",
    "    \n",
    "    def _get_internals(self, x, hp, kk):\n",
    "        # Actual forward computations\n",
    "        rint = self.Wih.matmul(x)\n",
    "        r = ((self.tvec[kk]*rint)>0)*rint + self.Whh.matmul(hp)\n",
    "        h = ((self.tvec[kk]*r)>0)*r\n",
    "        s = self.Woh.matmul(h)\n",
    "        y = torch.sigmoid(s)\n",
    "        \n",
    "        return rint, r, h, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d878f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, cell):\n",
    "        self.cell = cell\n",
    "    \n",
    "    def forward(self, xs, h0):\n",
    "        \"\"\"Perform the forward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep. The\n",
    "                first dimension corresponds to the number of timesteps.\n",
    "            h0 (Tensor): Initial hidden state.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: 2-D tensor of outputs for each timestep. The first\n",
    "                dimension corresponds to the number of timesteps.\n",
    "            Tensor: 2-D tensor of hidden states for each timestep plus\n",
    "                `h0`. The first dimension corresponds to the number of\n",
    "                timesteps.\n",
    "        \"\"\"\n",
    "        ys, hs = [], [h0]\n",
    "        for ii, x in enumerate(xs):\n",
    "            y, h = self.cell.forward(x, hs[-1],ii)\n",
    "            ys.append(y)\n",
    "            hs.append(h)\n",
    "        return torch.stack(ys), torch.stack(hs)\n",
    "    \n",
    "    def backward(self, ys_grad, xs, hs):\n",
    "        \"\"\"Perform the backward computation for all timesteps.\n",
    "        \n",
    "        Args:\n",
    "            ys_grad (Tensor): 2-D tensor of the gradients on outputs\n",
    "                for each timestep. The first dimension corresponds to\n",
    "                the number of timesteps.\n",
    "            xs (Tensor): 2-D tensor of inputs for each timestep that\n",
    "                was passed to `forward`.\n",
    "            hs (Tensor): 2-D tensor of hidden states that is returned\n",
    "                by `forward`.\n",
    "        \"\"\"\n",
    "        # For the last timestep, the gradient on r is zero\n",
    "        rn_grad = torch.zeros(self.cell.hid_dim)\n",
    "\n",
    "        for ii, (y_grad, x, hp) in enumerate(reversed(list(zip(ys_grad, xs, hs)))):\n",
    "            rn_grad  = cell.backward(y_grad, rn_grad, x, hp, n_classes-ii-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6aca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = RNNModule(input_dim, hidden_dim, output_dim, tvec)\n",
    "rnn = RNN(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590043bd",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2727e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_cross_entropy(predictions, targets, epsilon=1e-15):\n",
    "#     \"\"\"\n",
    "#     Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "#     and predictions. \n",
    "#     Input: predictions (N, k) ndarray\n",
    "#            targets (N, k) ndarray        \n",
    "#     Returns: scalar\n",
    "#     \"\"\"\n",
    "#     predictions = torch.clip(predictions, epsilon, 1. - epsilon)\n",
    "#     N = predictions.shape[0]\n",
    "#     ce = -torch.sum(targets*torch.log(predictions+1e-12))/N\n",
    "    \n",
    "#     return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e780b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(ys, ts):\n",
    "    return 0.5 * torch.sum((ys - ts)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33eb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note -- changed!!\n",
    "xs = image_batch\n",
    "hp = torch.zeros(cell.hid_dim)\n",
    "ts = label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31f8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc46c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, hs = rnn.forward(xs, hp)\n",
    "loss = compute_loss(ys, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e7380",
   "metadata": {},
   "source": [
    "#### PyTorch gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "053ad27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient of cross entropy w.r.t. yhat_k = (1/yhat_k)*y_k\n",
    "## gradient of mse w.r.t. y_hat = y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac4d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab2b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4054d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a8bce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f07a3",
   "metadata": {},
   "source": [
    "#### Manual gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64a3eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is obtained from our loss function\n",
    "# ys_grad = torch.matmul(label_batch,ys)\n",
    "ys_grad = ys - ts\n",
    "\n",
    "with torch.no_grad():  # required so PyTorch won't raise error\n",
    "    rnn.cell.zero_grad()\n",
    "    rnn.backward(ys_grad, xs, hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abca0f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.arange(1., 101.)\n",
    "v2 = torch.arange(1., 785.)\n",
    "torch.outer(v1,v2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fca57235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601539f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86292cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23dc2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fdad959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Woh_grad - rnn.cell.Woh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25345d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Whh_grad - rnn.cell.Whh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e38c20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.cell.Wih_grad - rnn.cell.Wih.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d9c49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Woh_grad - rnn.cell.Woh.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae1358ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Whh_grad - rnn.cell.Whh.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "604a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(rnn.cell.Wih_grad - rnn.cell.Wih.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11a9302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.sign(rnn.cell.Woh_grad) - torch.sign(rnn.cell.Woh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0adfa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (torch.abs(rnn.cell.Woh_grad - rnn.cell.Woh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bc22ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed total differences for Whh:  tensor([[ 5.7070,  9.4697,  3.8032,  ...,  5.8523,  3.8743, 11.8866],\n",
      "        [ 7.4038,  1.4343,  0.9471,  ...,  1.2072,  7.4682,  2.4491],\n",
      "        [ 0.0540,  9.5127,  0.3405,  ...,  1.9263,  0.2000, 12.8698],\n",
      "        ...,\n",
      "        [ 0.0540,  9.1172,  0.5374,  ...,  1.2078,  1.2888, 10.5332],\n",
      "        [ 5.7074,  9.9335,  4.3793,  ...,  5.8555,  3.8745, 13.3747],\n",
      "        [ 7.4038,  0.9750,  0.3730,  ...,  1.2070,  7.4677,  0.9696]])\n"
     ]
    }
   ],
   "source": [
    "print('Summed total differences for Whh: ',torch.abs(rnn.cell.Whh_grad - rnn.cell.Whh_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58429add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed total differences for Wih:  tensor([[2.9373, 2.9373, 2.9373,  ..., 2.9373, 2.9373, 2.9373],\n",
      "        [2.9390, 2.9390, 2.9390,  ..., 2.9390, 2.9390, 2.9390],\n",
      "        [3.6751, 3.6751, 3.6751,  ..., 3.6751, 3.6751, 3.6751],\n",
      "        ...,\n",
      "        [2.7510, 2.7510, 2.7510,  ..., 2.7510, 2.7510, 2.7510],\n",
      "        [4.2569, 4.2569, 4.2569,  ..., 4.2569, 4.2569, 4.2569],\n",
      "        [3.1666, 3.1666, 3.1666,  ..., 3.1666, 3.1666, 3.1666]])\n"
     ]
    }
   ],
   "source": [
    "print('Summed total differences for Wih: ',torch.abs(rnn.cell.Wih_grad - rnn.cell.Wih_grad_geb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "960fa83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Timestep signed differences for Wih: tensor([0.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "1\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "2\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "3\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "4\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "5\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "6\n",
      "Timestep signed differences for Wih: tensor([-2., -1.,  0.,  1.,  2.])\n",
      "Timestep signed differences for Whh: tensor([0., 2.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "7\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "8\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n",
      "9\n",
      "Timestep signed differences for Wih: tensor([-1.,  0.,  1.])\n",
      "Timestep signed differences for Whh: tensor([-1.,  0.])\n",
      "Timestep signed differences for Woh: tensor([0.])\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for tt in range(n_classes):\n",
    "    print(tt)\n",
    "    \n",
    "    rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_all[tt]==torch.clamp(rnn.cell.Wih_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_geb_all[tt]==torch.clamp(rnn.cell.Wih_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_all[tt]==torch.clamp(rnn.cell.Whh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_geb_all[tt]==torch.clamp(rnn.cell.Whh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_all[tt]==torch.clamp(rnn.cell.Woh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "    rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_geb_all[tt]==torch.clamp(rnn.cell.Woh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    print('Timestep signed differences for Wih:',torch.unique(torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt])))\n",
    "    print('Timestep signed differences for Whh:',torch.unique(torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt])))\n",
    "    print('Timestep signed differences for Woh:',torch.unique(torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt])))\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da65fa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Frac. 0 signed differences for Wih: 1.0\n",
      "Frac. 1 signed differences for Wih: 0.0\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.0\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "1\n",
      "Frac. 0 signed differences for Wih: 0.96\n",
      "Frac. 1 signed differences for Wih: 0.034846938775510206\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.005153061224489796\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "2\n",
      "Frac. 0 signed differences for Wih: 0.87\n",
      "Frac. 1 signed differences for Wih: 0.11325255102040817\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.016747448979591836\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "3\n",
      "Frac. 0 signed differences for Wih: 0.77\n",
      "Frac. 1 signed differences for Wih: 0.20036989795918367\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.029630102040816328\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "4\n",
      "Frac. 0 signed differences for Wih: 0.82\n",
      "Frac. 1 signed differences for Wih: 0.15681122448979593\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.023188775510204082\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "5\n",
      "Frac. 0 signed differences for Wih: 0.7599362244897959\n",
      "Frac. 1 signed differences for Wih: 0.2091454081632653\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.030918367346938775\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "6\n",
      "Frac. 0 signed differences for Wih: 0.48\n",
      "Frac. 1 signed differences for Wih: 0.029630102040816328\n",
      "Frac. 2 signed differences for Wih: 0.03735969387755102\n",
      "Frac. -1 signed differences for Wih: 0.20073979591836735\n",
      "Frac. -2 signed differences for Wih: 0.2522704081632653\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 0.75\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.25\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "7\n",
      "Frac. 0 signed differences for Wih: 0.7490816326530613\n",
      "Frac. 1 signed differences for Wih: 0.21849489795918367\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.0324234693877551\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "8\n",
      "Frac. 0 signed differences for Wih: 0.7192219387755102\n",
      "Frac. 1 signed differences for Wih: 0.24040816326530612\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.04036989795918367\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 1.0\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.0\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n",
      "9\n",
      "Frac. 0 signed differences for Wih: 0.48\n",
      "Frac. 1 signed differences for Wih: 0.4530102040816327\n",
      "Frac. 2 signed differences for Wih: 0.0\n",
      "Frac. -1 signed differences for Wih: 0.06698979591836735\n",
      "Frac. -2 signed differences for Wih: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Whh: 0.8967\n",
      "Frac. 1 signed differences for Whh: 0.0\n",
      "Frac. 2 signed differences for Whh: 0.0\n",
      "Frac. -1 signed differences for Whh: 0.1033\n",
      "Frac. -2 signed differences for Whh: 0.0\n",
      "-----------------------\n",
      "Frac. 0 signed differences for Woh: 1.0\n",
      "Frac. 1 signed differences for Woh: 0.0\n",
      "Frac. 2 signed differences for Woh: 0.0\n",
      "Frac. -1 signed differences for Woh: 0.0\n",
      "Frac. -2 signed differences for Woh: 0.0\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "for tt in range(n_classes):\n",
    "    \n",
    "    print(tt)\n",
    "    \n",
    "    totWih = np.prod(rnn.cell.Wih_grad_all[tt].shape)\n",
    "    totWhh = np.prod(rnn.cell.Whh_grad_all[tt].shape)\n",
    "    totWoh = np.prod(rnn.cell.Woh_grad_all[tt].shape)\n",
    "    \n",
    "#     rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_all[tt]==torch.clamp(rnn.cell.Wih_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "#     rnn.cell.Wih_grad_all[tt][rnn.cell.Wih_grad_geb_all[tt]==torch.clamp(rnn.cell.Wih_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "#     rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_all[tt]==torch.clamp(rnn.cell.Whh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "#     rnn.cell.Whh_grad_all[tt][rnn.cell.Whh_grad_geb_all[tt]==torch.clamp(rnn.cell.Whh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "#     rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_all[tt]==torch.clamp(rnn.cell.Woh_grad_all[tt], -1e-6 , 1e-6)]=0\n",
    "#     rnn.cell.Woh_grad_all[tt][rnn.cell.Woh_grad_geb_all[tt]==torch.clamp(rnn.cell.Woh_grad_geb_all[tt], -1e-6 , 1e-6)]=0\n",
    "    \n",
    "    num0_Wih = len(np.where((torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt]))==0)[0])\n",
    "    num1_Wih = len(np.where((torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt]))==1)[0])\n",
    "    num2_Wih = len(np.where((torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt]))==2)[0])\n",
    "    numm1_Wih = len(np.where((torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt]))==-1)[0])\n",
    "    numm2_Wih = len(np.where((torch.sign(rnn.cell.Wih_grad_all[tt]) - torch.sign(rnn.cell.Wih_grad_geb_all[tt]))==-2)[0])\n",
    "    \n",
    "    num0_Whh = len(np.where((torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt]))==0)[0])\n",
    "    num1_Whh = len(np.where((torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt]))==1)[0])\n",
    "    num2_Whh = len(np.where((torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt]))==2)[0])\n",
    "    numm1_Whh = len(np.where((torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt]))==-1)[0])\n",
    "    numm2_Whh = len(np.where((torch.sign(rnn.cell.Whh_grad_all[tt]) - torch.sign(rnn.cell.Whh_grad_geb_all[tt]))==-2)[0])\n",
    "    \n",
    "    num0_Woh = len(np.where((torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt]))==0)[0])\n",
    "    num1_Woh = len(np.where((torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt]))==1)[0])\n",
    "    num2_Woh = len(np.where((torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt]))==2)[0])\n",
    "    numm1_Woh = len(np.where((torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt]))==-1)[0])\n",
    "    numm2_Woh = len(np.where((torch.sign(rnn.cell.Woh_grad_all[tt]) - torch.sign(rnn.cell.Woh_grad_geb_all[tt]))==-2)[0])\n",
    "    \n",
    "    print('Frac. 0 signed differences for Wih:',num0_Wih/totWih)\n",
    "    print('Frac. 1 signed differences for Wih:',num1_Wih/totWih)\n",
    "    print('Frac. 2 signed differences for Wih:',num2_Wih/totWih)\n",
    "    print('Frac. -1 signed differences for Wih:',numm1_Wih/totWih)\n",
    "    print('Frac. -2 signed differences for Wih:',numm2_Wih/totWih)\n",
    "    print('-----------------------')\n",
    "    print('Frac. 0 signed differences for Whh:',num0_Whh/totWhh)\n",
    "    print('Frac. 1 signed differences for Whh:',num1_Whh/totWhh)\n",
    "    print('Frac. 2 signed differences for Whh:',num2_Whh/totWhh)\n",
    "    print('Frac. -1 signed differences for Whh:',numm1_Whh/totWhh)\n",
    "    print('Frac. -2 signed differences for Whh:',numm2_Whh/totWhh)\n",
    "    print('-----------------------')\n",
    "    print('Frac. 0 signed differences for Woh:',num0_Woh/totWoh)\n",
    "    print('Frac. 1 signed differences for Woh:',num1_Woh/totWoh)\n",
    "    print('Frac. 2 signed differences for Woh:',num2_Woh/totWoh)\n",
    "    print('Frac. -1 signed differences for Woh:',numm1_Woh/totWoh)\n",
    "    print('Frac. -2 signed differences for Woh:',numm2_Woh/totWoh)\n",
    "    print('=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d4262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
